{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06284050",
   "metadata": {},
   "source": [
    "# Power Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d355fed3",
   "metadata": {},
   "source": [
    "Standard is a format of the trading card game Magic: the Gathering in which players are only allowed to use cards from the past few years of set releases. Because of the limited card pool, new additions to the format via new set releases often have a dramatic effect, but it can be hard to predict what cards will be powerful when they release. \n",
    "\n",
    "This project seeks to rate Magic: the Gathering cards for power level in the current Standard format on a scale from 1-5, with 1 being completely unplayable and irrelevant, and 5 being format warping and broken. In order to do this effectively, I used two methods, and compared them to find the best approach. \n",
    "\n",
    "The first was to use OpenAI's API and make API calls to gpt-4-turbo. When given a card name, this module will fetch the relevant card data from the Scryfall API, including Oracle text, mana cost, and types. The data is then given to the API with a carefully engineered prompt, and it responds with a rating and rational for that rating.\n",
    "\n",
    "The second approach was to download a pretrained LLM from huggingface and then fine-tune it on a list of standard-legal Magic: the Gathering cards each paired with a power rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e872540f",
   "metadata": {},
   "source": [
    "#### Scryfall fetch code\n",
    "\n",
    "Method to fetch card data from scryfall. It is stored in a dictionary and returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9b9ce177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_card_info(card_name):\n",
    "        \"\"\"\n",
    "        Fetch Magic: The Gathering card information from Scryfall API\n",
    "        \n",
    "        Args:\n",
    "            card_name: Name of the MTG card to search for\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with card information or None if card not found\n",
    "        \"\"\"\n",
    "        # URL encode the card name for the API request\n",
    "        url = f\"https://api.scryfall.com/cards/named?exact={card_name}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url).json()\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            if response:\n",
    "                card_data = response\n",
    "                \n",
    "                # Extract the requested information\n",
    "                card_info = {\n",
    "                    'name': card_data.get('name'),\n",
    "                    'mana_cost': card_data.get('mana_cost'),\n",
    "                    'types': card_data.get('type_line'),\n",
    "                    'oracle_text': card_data.get('oracle_text'),\n",
    "                    'power': card_data.get('power'),\n",
    "                    'toughness': card_data.get('toughness'),\n",
    "                    'loyalty': card_data.get('loyalty')\n",
    "                }\n",
    "                \n",
    "                return card_info\n",
    "            else:\n",
    "                print(f\"Error: Could not find card named '{card_name}'\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6fee89",
   "metadata": {},
   "source": [
    "## API Call\n",
    "\n",
    "This section contains the code and prompts for the API Call section of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc829a4",
   "metadata": {},
   "source": [
    "### Imports\n",
    "* requests: easy get requests from Scryfall API\n",
    "* openai: API library\n",
    "* Markdown, display: Library for displaying markdown output from the API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6fee49e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import openai\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02678ce5",
   "metadata": {},
   "source": [
    "#### Get card info\n",
    "This method retrieves relevant information about cards from Scryfall, which is a database containing Magic: the Gathering cards and the information about them. It stores the information in a map (card_info), which links the information to appropriate names and returns the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "38c96f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_card_info(card_name):\n",
    "        \"\"\"\n",
    "        Fetch Magic: The Gathering card information from Scryfall API\n",
    "        \n",
    "        Args:\n",
    "            card_name: Name of the MTG card to search for\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with card information or None if card not found\n",
    "        \"\"\"\n",
    "        # URL encode the card name for the API request\n",
    "        url = f\"https://api.scryfall.com/cards/named?exact={card_name}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url).json()\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            if response:\n",
    "                card_data = response\n",
    "                \n",
    "                # Extract the requested information\n",
    "                card_info = {\n",
    "                    'image_uris': card_data.get('image_uris'),\n",
    "                    'name': card_data.get('name'),\n",
    "                    'mana_cost': card_data.get('mana_cost'),\n",
    "                    'types': card_data.get('type_line'),\n",
    "                    'oracle_text': card_data.get('oracle_text'),\n",
    "                    'power': card_data.get('power'),\n",
    "                    'toughness': card_data.get('toughness'),\n",
    "                    'loyalty': card_data.get('loyalty')\n",
    "                }\n",
    "                \n",
    "                return card_info\n",
    "            else:\n",
    "                print(f\"Error: Could not find card named '{card_name}'\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c93e233",
   "metadata": {},
   "source": [
    "#### System Prompt\n",
    "\n",
    "This is the system prompt, which is given to the API before being prompted with a card. First, it outlines the task, which is to rate a card on a scale from 1-5 in the standard format, which is outlined and briefly described. Then each number on the scale is given a definition, and it's meaning is outlined. The model is told what the analysis of the score should include, and then there are 5 examples, 1 for each tier of power. This gives the model the ability to learn in-context, and provides a baseline for the model to compare cards it is asked to rate to. I attempted to choose a diverse set of cards to use as examples so that the model would have information about all types of cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3addd24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a Magic: The Gathering expert specializing in evaluating cards for Standard format play.\n",
    "    The best standard decks in the foramt are highly value centered and full of removal and powerful threats.\n",
    "    Analyze the provided card based on its mana cost, types, oracle text, and power/toughness if applicable.\n",
    "    Rate the card on a scale of 1-5 where:\n",
    "    \n",
    "    1: Unplayable in every situation, and outclassed by other cards\n",
    "    2: Of only average strength. Playable in niche archetypes or for specific sideboard uses\n",
    "    3: Strong cards that can be played in a variety of decks, but mainly as a supporting card\n",
    "    4: Exceptionally strong cards that inspire deck archetypes and provide lots of value on their own\n",
    "    5: Broken and format-warping card than defines the metagame\n",
    "    \n",
    "    Your analysis should include:\n",
    "    1. Power Rating (1-5)\n",
    "    2. Strengths of the card\n",
    "    3. Weaknesses or limitations\n",
    "    \n",
    "    Here is an example of a 1: \n",
    "    Card Name: \"Air Marshal\"\n",
    "    Mana Cost: 1U\n",
    "    Types: Creature - Human Soldier\n",
    "    Oracle Text: 3: Target Soldier gains flying until end of turn.\n",
    "    Power: 2\n",
    "    Toughness: 1\n",
    "    \n",
    "    This card provides a mediocre body for its cost, and the cost of 3 mana to give a creature flying is far too high for the effect.\n",
    "    It is outclassed by many other cards in the format and does not provide enough value to be worth playing.\n",
    "    \n",
    "    Here is an example of a 2:\n",
    "    Card Name: \"Abrade\"\n",
    "    Mana Cost: 1R\n",
    "    Types: Instant\n",
    "    Oracle text: Choose one - Abrade deals 3 damage to target creature; or destroy target artifact.\n",
    "    Power: N/A\n",
    "    Toughness: N/A\n",
    "    This card provides decent utility, but at 2 mana it doesn't provide enough value to be worth playing in most decks.\n",
    "    \n",
    "    Here is an example of a 3:\n",
    "    Card Name: Amalia Benavides Aguirre\n",
    "    Mana Cost: WB\n",
    "    Types: Legendary Creature - Vampire Scout\n",
    "    Oracle text: Ward - Pay 3 life. Whenever you gain life, Amalia Benavides Aguirre explores. Then destroy all other creatures if its power is exactly 20. (To have this creature explore, reveal the top card of your library. Put that card into your hand if it is a land. Otherwise, put a +1/+1 counter on this creature, then put the card back or put it into your graveyard.)\n",
    "    Power: 2\n",
    "    Toughness: 2\n",
    "    This card provides excellent value for life gain deck via growing power and toughness and lots of card selection. Becuase of this, it is a powerful addition to decks that gain life incrementally.\n",
    "    \n",
    "    Here is an example of a 4: \n",
    "    Card Name: \"Overlord of the Hauntwoods\"\n",
    "    Mana Cost: 3GG\n",
    "    Types: Enchantment Creature - Avatar Horror\n",
    "    Oracle Text: Impending 4—1GreenGreen (If you cast this spell for its impending cost, it enters with four time counters and isn't a creature until the last is removed. At the beginning of your end step, remove a time counter from it.) Whenever this permanent enters or attacks, create a tapped colorless land token named Everywhere that is every basic land type.\n",
    "    This card provides immense value for ramp decks and domain decks via its ability to create a land token that is every basic land type. Furthermore, it is flexible in that it can be played for full price as a creature, or early for its impending cost.\n",
    "    This card is very powerful, and finds its way into many decks.\n",
    "    \n",
    "    Here is an example of a 5:\n",
    "    Card Name: \"Up the Beanstalk\"\n",
    "    Mana Cost: 1G\n",
    "    Types: Enchantment\n",
    "    Oracle text: When Up the Beanstalk enters the battlefield and whenever you cast a spell with mana value 5 or greater, draw a card.\n",
    "    Power: N/A\n",
    "    Toughness: N/A\n",
    "    \n",
    "    This card provides immediate value upon entering via drawing a card, and provides powerful repeated value throughout the game.\n",
    "    Furthermore, because of synergies with other strong cards in the format, it is very easy to trigger the card draw effect.\n",
    "    This card is the reason many of the best decks exist, and is a format-defining card.\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed0fc60",
   "metadata": {},
   "source": [
    "#### OpenAI API key\n",
    "\n",
    "Insert OpenAI API key here to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "923bfd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd74305",
   "metadata": {},
   "source": [
    "#### API Call\n",
    "\n",
    "This method calls the LLM API. It is passed the map of card information, and plugs it into the user prompt to elicit a response. 1000 max token is chosen because as this task can be quite complicated, sometimes extensive analysis is required. A temperature of 0.2 ensures that the model does not provide inaccurate ratings that were not the most likely option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5d7e94e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_card(card_info):\n",
    "    try:\n",
    "        client = openai.OpenAI(api_key=API_KEY)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Please analyze this Magic: The Gathering card:\\n\\nName: {card_info['name']}\\nMana Cost: {card_info['mana_cost']}\\nTypes: {card_info['types']}\\nOracle Text: {card_info['oracle_text']}\\nPower/Toughness: {card_info['power']}/{card_info['toughness']}\\nLoyalty: {card_info['loyalty']}\\n\\n\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658144bc",
   "metadata": {},
   "source": [
    "#### Driver code\n",
    "\n",
    "This cell calls each method defined above and prints the LLM's response, which is given in Markdown format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7f0f4d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up the Beanstalk\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Power Rating: 5\n",
       "\n",
       "Strengths of the card:\n",
       "1. **Immediate Value**: \"Up the Beanstalk\" provides immediate value upon entering the battlefield by allowing the player to draw a card. This helps mitigate the cost of playing the enchantment by replacing itself in the player's hand, ensuring that the player does not lose card advantage.\n",
       "2. **Recurring Value**: The ability to draw a card whenever a spell with mana value 5 or greater is cast can lead to significant card advantage over the course of a game. This is particularly strong in formats or decks that regularly play high-cost spells, making it a powerful engine in ramp or big mana decks.\n",
       "3. **Low Mana Cost**: At only 2 mana, this enchantment is very easy to incorporate into a variety of game plans. Its low cost allows it to be played early in the game, setting up card draw engines as soon as larger spells start being cast.\n",
       "4. **Deck Synergy**: This card synergizes well with decks that focus on casting large spells, such as ramp decks or control decks that stabilize and then play high-impact spells. It can also be a key piece in decks that have ways to cheat on mana costs, effectively drawing cards off discounted spells.\n",
       "\n",
       "Weaknesses or limitations:\n",
       "1. **Dependency on High-Cost Spells**: The primary limitation of \"Up the Beanstalk\" is that its value is contingent upon being able to cast spells with a mana value of 5 or greater. In decks without a sufficient number of high-cost spells, or in games where the player is prevented from reaching sufficient mana resources, the enchantment's utility may be significantly diminished.\n",
       "2. **Vulnerability to Removal**: As an enchantment, it is susceptible to any form of enchantment removal. If removed before it can generate sufficient value, it could lead to a tempo loss.\n",
       "3. **Potential for Dead Draws**: In situations where the player is behind and needs immediate impact on the board, drawing this card late in the game without the setup for high-cost spells can be less impactful compared to other potential draws.\n",
       "\n",
       "Overall, \"Up the Beanstalk\" is a powerful card that can define the structure and strategy of the decks it's included in, promoting a game plan centered around casting high-cost, high-impact spells and generating substantial card advantage. Its ability to replace itself upon entry and continuously provide value makes it a format-defining card in the right environment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abrade\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Power Rating: 4\n",
       "\n",
       "Strengths of the card:\n",
       "1. **Flexibility**: Abrade's ability to choose between dealing 3 damage to a creature or destroying an artifact makes it highly versatile. This flexibility allows it to be relevant in multiple game scenarios, making it a valuable inclusion in many red decks.\n",
       "2. **Cost Efficiency**: At a cost of only {1}{R}, Abrade is very mana-efficient. This low cost enables players to remain reactive and flexible with their mana, potentially casting other spells in the same turn.\n",
       "3. **Instant Speed**: Being an instant allows Abrade to be used both defensively and offensively, providing options to disrupt opponent's plays during their turn or to clear the way for attacks during your turn.\n",
       "4. **Meta Relevance**: In a format where artifacts can be prevalent or where creatures with toughness 3 or less are common, Abrade shines by providing an answer to key threats or utility pieces.\n",
       "\n",
       "Weaknesses or limitations:\n",
       "1. **Damage Limitation**: The damage cap of 3 means that Abrade cannot deal with larger creatures that are common in some Standard environments. This limits its effectiveness against decks with high-toughness creatures.\n",
       "2. **No Player Targeting**: Abrade cannot target players, which restricts its utility compared to some other damage spells that can also be directed at opponents for direct damage, potentially missing some opportunities for lethal plays.\n",
       "\n",
       "Overall, Abrade's strengths in flexibility, cost, and instant speed make it an exceptionally strong card that can fit into various red or multi-colored decks, influencing its widespread use in the Standard format. Its ability to answer multiple types of threats at a low cost is a significant advantage, making it a staple in decks that include red."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abzan Monument\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Power Rating: 3\n",
       "\n",
       "Strengths of the card:\n",
       "1. **Mana Efficiency and Ramp**: Abzan Monument has a low initial mana cost of {2}, making it an accessible play in the early game. Its ability to fetch a basic Plains, Swamp, or Forest upon entering the battlefield aids in mana fixing and slight ramping, which is crucial in a three-color deck.\n",
       "2. **Flexibility in Token Generation**: The second ability to create a white Spirit creature token based on the greatest toughness among creatures you control can be a powerful late-game play. This ability allows for the creation of potentially large blockers or attackers, depending on the state of the board and the creatures you control.\n",
       "3. **Deck Synergy**: This card fits well in decks that focus on creature toughness and can synergize with strategies that benefit from having artifacts or sacrificing permanents for value.\n",
       "\n",
       "Weaknesses or limitations:\n",
       "1. **Activation Restriction**: The token-generating ability can only be activated as a sorcery, which limits its utility in response to opponents' actions during their turns. This reduces the strategic flexibility of the card.\n",
       "2. **Dependency on Board State**: The value derived from the token-generating ability heavily depends on the presence of creatures with high toughness on the board. Without such creatures, the ability becomes less impactful.\n",
       "3. **One-Time Use**: The need to sacrifice Abzan Monument to use its second ability means it's a one-time effect, requiring careful timing and consideration to maximize its benefit.\n",
       "\n",
       "Overall, Abzan Monument is a solid card in the right deck, particularly in those built around Abzan colors (White, Black, Green) with a focus on creature toughness. Its early game utility for mana fixing and late-game potential for creating significant threats gives it a decent level of versatility and strategic depth, making it a strong supporting card in the Standard format."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "card_to_analyze1 = \"Up the Beanstalk\"\n",
    "info1 = get_card_info(card_to_analyze1)\n",
    "rating1 = analyze_card(info1)\n",
    "print(card_to_analyze1)\n",
    "display(Markdown(rating1))\n",
    "\n",
    "card_to_analyze2 = \"Abrade\"\n",
    "info2 = get_card_info(card_to_analyze2)\n",
    "rating2 = analyze_card(info2)\n",
    "print(card_to_analyze2)\n",
    "display(Markdown(rating2))\n",
    "\n",
    "card_to_analyze3 = \"Abzan Monument\"\n",
    "info3 = get_card_info(card_to_analyze3)\n",
    "rating3 = analyze_card(info3)\n",
    "print(card_to_analyze3)\n",
    "display(Markdown(rating3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca47c805",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Overall, the model performs quite well with most cards. It gives a reasonable answer that agrees with my analysis and provides quality analysis of why the card is in the spot that it chose.\n",
    "\n",
    "Problems: The model often gives cards a rating higher than what is correct. For example, as seen in the above examples, it rates Abrade a 4 and Abzan Monument a 3. I would rate them both as 2, being playable in specific decks. The model tends to give cards that have flexible uses hgiher scores. Abrade has two modes that can be chosen, and Abzan Monument has a mode that can be activated in the late game to provide a powerful creature. It references this flexibility in it's analysis as one of the reasons for it's rating. This could potentially be solved with more additions to the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b306df7b",
   "metadata": {},
   "source": [
    "## Fine Tuned LLM\n",
    "\n",
    "This section contains the code and output for the fine-tuned distilbert model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8829c3",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "* Pandas, datasets: data tools\n",
    "* predictor, transformers, peft, torch: LLM and training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4fa5dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import predictor\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d203531b",
   "metadata": {},
   "source": [
    "#### Login to Huggingface\n",
    "\n",
    "Provide a huggingface key to load distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a9155a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6bd2734b8c4358a3c69abd1c29c3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa182ab",
   "metadata": {},
   "source": [
    "#### Model Hyperparameters\n",
    "\n",
    "Relevant decisions:\n",
    "* Model: distilbert-base-uncased chosen for size and classification prowess\n",
    "* Learning rate: low learning rate chosen to help avoid overfitting on the small dataset\n",
    "* Batch size: small batch size to help avoid overfitting\n",
    "* Low epoch number for the same reason as above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7db3a554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configuring model...\n"
     ]
    }
   ],
   "source": [
    "print(\"configuring model...\")\n",
    "MODEL_NAME = \"distilbert/distilbert-base-uncased\"\n",
    "OUTPUT_DIR = \"./mtg_rating_model\"\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "MAX_LENGTH = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8cc992",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "This dataset is 100 cards along with ratings chosen by me based on my experience with the game and also upon the MTG golfish meta page (https://www.mtggoldfish.com/metagame/standard/full#paper). The cards were semi-randomly chosen from all the cards available in the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ddb15b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = [\n",
    "        \"Adventuring Gear\",\n",
    "        \"Basilisk Collar\",\n",
    "        \"Bloodthorn Flail\",\n",
    "        \"Carnelian Orb of Dragonkind\",\n",
    "        \"Carrot Cake\",\n",
    "        \"Cori-Steel Cutter\",\n",
    "        \"Gilded Lotus\",\n",
    "        \"Golden Argosy\",\n",
    "        \"Monument to Endurance\",\n",
    "        \"Perilous Snare\",\n",
    "        \"Racers' Scoreboard\",\n",
    "        \"Rope\",\n",
    "        \"Runaway Boulder\",\n",
    "        \"Abhorrent Oculus\",\n",
    "        \"Ajani's Pridemate\",\n",
    "        \"Ash, Party Crasher\",\n",
    "        \"Ball Lightning\",\n",
    "        \"Beza, the Bounding Spring\",\n",
    "        \"Bloodghast\",\n",
    "        \"Boulderborn Dragon\",\n",
    "        \"Brightblade Stoat\",\n",
    "        \"Defiler of Vigor\",\n",
    "        \"Diregraf Ghoul\",\n",
    "        \"Edgewall Pack\",\n",
    "        \"Elvish Archdruid\",\n",
    "        \"Essence Channeler\",\n",
    "        \"Evolved Sleeper\",\n",
    "        \"Fang Guardian\",\n",
    "        \"Fangkeeper's Familiar\",\n",
    "        \"Friendly Teddy\",\n",
    "        \"Fynn, the Fangbearer\",\n",
    "        \"Greedy Freebooter\",\n",
    "        \"Halo-Charged Skaab\",\n",
    "        \"Haughty Djinn\",\n",
    "        \"Heartfire Hero\",\n",
    "        \"Hinterland Sanctifier\",\n",
    "        \"Ingenious Leonin\",\n",
    "        \"Iridescent Vinelasher\",\n",
    "        \"Jolly Gerbils\",\n",
    "        \"Kiora, the Rising Tide\",\n",
    "        \"Knight-Errant of Eos\",\n",
    "        \"Kraul Whipcracker\",\n",
    "        \"Llanowar Elves\",\n",
    "        \"Manifold Mouse\",\n",
    "        \"Mintstrosity\",\n",
    "        \"Nurturing Pixie\",\n",
    "        \"Overlord of the Hauntwoods\",\n",
    "        \"Overlord of the Boilerbilges\",\n",
    "        \"Pride of the Road\",\n",
    "        \"Rankle and Torbran\",\n",
    "        \"Savage Ventmaw\",\n",
    "        \"Savannah Lions\",\n",
    "        \"Screaming Nemesis\",\n",
    "        \"Severance Priest\",\n",
    "        \"Sire of Seven Deaths\",\n",
    "        \"Skirmish Rhino\",\n",
    "        \"Spiteful Hexmage\",\n",
    "        \"Tangled Colony\",\n",
    "        \"Up the Beanstalk\",\n",
    "        \"Caretaker's Talent\",\n",
    "        \"Colossification\",\n",
    "        \"Disturbing Mirth\",\n",
    "        \"Leyline of Resonance\",\n",
    "        \"Lost in the Maze\",\n",
    "        \"Nahiri's Resolve\",\n",
    "        \"Nowhere to Run\",\n",
    "        \"Phyrexian Arena\",\n",
    "        \"Tribute to the World Tree\",\n",
    "        \"Monstrous Rage\",\n",
    "        \"Abrade\",\n",
    "        \"Aetherize\",\n",
    "        \"Bite Down\",\n",
    "        \"Flame Lash\",\n",
    "        \"Get Out\",\n",
    "        \"Get Lost\",\n",
    "        \"This Town Ain't Big Enough\",\n",
    "        \"Negate\",\n",
    "        \"On the Job\",\n",
    "        \"Opt\",\n",
    "        \"Rat Out\",\n",
    "        \"Refute\",\n",
    "        \"Ride's End\",\n",
    "        \"Slick Sequence\",\n",
    "        \"Steer Clear\",\n",
    "        \"Torch the Tower\",\n",
    "        \"Abuelo's Awakening\",\n",
    "        \"Boltwave\",\n",
    "        \"Captain's Call\",\n",
    "        \"Deathmark\",\n",
    "        \"Excavation Explosion\",\n",
    "        \"Exorcise\",\n",
    "        \"Feed the Swarm\",\n",
    "        \"Jailbreak Scheme\",\n",
    "        \"Lunar Insight\",\n",
    "        \"Maelstrom Pulse\",\n",
    "        \"Pyroclasm\",\n",
    "        \"Rankle's Prank\",\n",
    "        \"Zombify\",\n",
    "        \"Slime Against Humanity\",\n",
    "        \"Sunfall\"\n",
    "    ]\n",
    "\n",
    "ratings = [1, 2, 1, 2, 3, 4, 2, 2, 4, 3, 1, 2, 1, 4, 2, 2, 2, 4, 3, 1, 2, 3, 2, 2, 3, 3, 2, 1, 3, 1, 3, 3, 2, 3, 4, 3, 1, 3, 1, 3, 4, 2, 4, 3, 2, 3, 3, 4, 1, 2, 2, 2, 3, 2, 2, 3, 2, 2, 4, 1, 3, 4, 1, 1, 3, 3, 3, 5, 2, 3, 2, 1, 3, 3, 5, 3, 1, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 1, 2, 2, 1, 2, 3, 3, 2, 2, 4, 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea800b7e",
   "metadata": {},
   "source": [
    "##### Scryfall fetch\n",
    "\n",
    "Below code fetches card data from the Scryfall API, concatenates it into a string, and adds it to the cards_with_data list. Final data is stored in the dataset_df dataframe for tokenization later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "237cc35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cards_with_data = []\n",
    "for card in cards:\n",
    "    card_info = predictor.card_utils.get_card_info(card)\n",
    "    with_data = f\"Name: {card_info['name']}\\nMana Cost: {card_info['mana_cost']}\\nTypes: {card_info['types']}\\nOracle Text: {card_info['oracle_text']}\\nPower/Toughness: {card_info['power']}/{card_info['toughness']}\\nLoyalty: {card_info['loyalty']}\\n\\n.\"\n",
    "    cards_with_data.append(with_data)\n",
    "dataset_df = pd.DataFrame({\"card_text\": cards_with_data, \"rating\": ratings})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e75c9d5",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "Tokenizer function that, when passed the data and a tokenizer, will tokenize the inputs for the data and return it. Uses the distilbert tokenizer from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ed4301d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize the data for sequence classification\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    \"\"\"Tokenize examples for sequence classification\"\"\"\n",
    "    # Format the examples as input text\n",
    "    texts = [\n",
    "        f\"Rate the Magic: The Gathering card '{card_text}' on a scale from 1 to 5 where 1 is irrelevant to the current standard format and 5 is format-warping.\"\n",
    "        for card_text in examples[\"card_text\"]\n",
    "    ]\n",
    "    \n",
    "    # Tokenize with padding and truncation\n",
    "    tokenized = tokenizer(\n",
    "        texts, \n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Convert ratings to labels (subtract 1 to make labels 0-4 instead of 1-5)\n",
    "    tokenized[\"labels\"] = [label - 1 for label in examples[\"rating\"]]\n",
    "    \n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf77ed2",
   "metadata": {},
   "source": [
    "### Driver Code\n",
    "\n",
    "This is the main driver code that will run all of the functions defined above, and execute fine-tuning of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c5fe4",
   "metadata": {},
   "source": [
    "##### Split into training and test data\n",
    "\n",
    "Using a 80/20 split, split the dataset into training and validation dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fcc0b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(dataset_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0c2c6",
   "metadata": {},
   "source": [
    "##### Load model\n",
    "\n",
    "Load the distilbert-base-uncased model from pretrained, load the tokenizer, and declare our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c62a49dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distilbert/distilbert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "# Make sure the tokenizer has a pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "# Load the model with num_labels=5 for the 1-5 rating scale\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=5,  # 5 classes (ratings 1-5)\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50251ce6",
   "metadata": {},
   "source": [
    "##### Create peft config and apply to model\n",
    "\n",
    "Peft is a set of fine-tuning techniques that is aimed to adapt a pre-trained model to a new task while only updating a small amount of parameters. Using peft and LoRA, I can easily train the model on my desktop GPU, and even a laptop CPU should be able to handle training the model using this method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "70ba1f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,036,805 || all params: 67,994,122 || trainable%: 1.5248\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    inference_mode=False,\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence classification task\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"o_lin\"]  # Typical attention modules\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297e4921",
   "metadata": {},
   "source": [
    "##### Tokenize the dataset and process into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "31fea4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759082a8f8de43f8a13e1de8b948c806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c3286a150b4db48a73c8fdc5d5d756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_dataset(examples):\n",
    "    return tokenize_function(examples, tokenizer)\n",
    "    \n",
    "# Process datasets with batched=True for efficiency\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "    \n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950386a3",
   "metadata": {},
   "source": [
    "##### Define training arguments and trainer using variables declared above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5c86c607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    push_to_hub=False,\n",
    "    fp16=False,\n",
    ")\n",
    "   \n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff5d864",
   "metadata": {},
   "source": [
    "##### Train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bbc63354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.582300</td>\n",
       "      <td>1.548437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.586600</td>\n",
       "      <td>1.607031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.564200</td>\n",
       "      <td>1.689844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.398700</td>\n",
       "      <td>1.681250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.479900</td>\n",
       "      <td>1.651562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./mtg_rating_model...\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting fine-tuning...\")\n",
    "trainer.train()\n",
    "    \n",
    "# Save the model\n",
    "print(f\"Saving model to {OUTPUT_DIR}...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "        \n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce7230a",
   "metadata": {},
   "source": [
    "### Predict card rating\n",
    "\n",
    "This function predicts the rating for a card when passed the card name, a model, and a tokenizer. It calls the scryfall API for the card data, tokenizes it, passes it to the model, and returns the output as a rating and probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "62da7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_card_rating(card_name, model, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Predict the rating for a specific Magic: The Gathering card\n",
    "    \n",
    "    Args:\n",
    "        card_name: Name of the card to rate\n",
    "        model: The fine-tuned model\n",
    "        tokenizer: The tokenizer for the model\n",
    "        max_length: Maximum sequence length\n",
    "        \n",
    "    Returns:\n",
    "        rating: Predicted rating (1-5)\n",
    "        confidence: Confidence scores for each class\n",
    "    \"\"\"\n",
    "    # Get card information\n",
    "    try:\n",
    "        card_info = get_card_info(card_name)\n",
    "        card_text = f\"Name: {card_info['name']}\\nMana Cost: {card_info['mana_cost']}\\nTypes: {card_info['types']}\\nOracle Text: {card_info['oracle_text']}\\nPower/Toughness: {card_info['power']}/{card_info['toughness']}\\nLoyalty: {card_info['loyalty']}\\n\\n.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching card info: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Format the prompt as it was during training\n",
    "    prompt = f\"Rate the Magic: The Gathering card '{card_text}' on a scale from 1 to 5 where 1 is irrelevant to the current standard format and 5 is format-warping.\"\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=max_length, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get predicted class and confidence scores\n",
    "    logits = outputs.logits.to(torch.float32)\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)[0]\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    \n",
    "    # Convert back to 1-5 scale (since model was trained on 0-4 labels)\n",
    "    rating = predicted_class + 1\n",
    "    \n",
    "    # Convert probabilities to a regular list\n",
    "    confidence_scores = probabilities.cpu().numpy().tolist()\n",
    "    \n",
    "    return rating, confidence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1203496",
   "metadata": {},
   "source": [
    "### Run Fine-Tuned Model\n",
    "\n",
    "Here I will pass some example cards to the fine-tuned model to see how it does. The code will call the function above and print the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cfd5fe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Card: Abyssal Gorestalker\n",
      "Predicted Rating: 2/5\n",
      "Confidence: ['14.3%', '45.9%', '34.0%', '4.8%', '0.9%']\n",
      "-----------------------\n",
      "Card: Adventuring Gear\n",
      "Predicted Rating: 3/5\n",
      "Confidence: ['28.3%', '22.5%', '45.9%', '2.5%', '0.8%']\n",
      "-----------------------\n",
      "Card: Monstrous Rage\n",
      "Predicted Rating: 2/5\n",
      "Confidence: ['3.5%', '91.6%', '2.3%', '1.9%', '0.5%']\n",
      "-----------------------\n",
      "Card: Phyrexian Arena\n",
      "Predicted Rating: 3/5\n",
      "Confidence: ['26.7%', '4.2%', '57.7%', '10.6%', '0.8%']\n",
      "-----------------------\n",
      "Card: Abrade\n",
      "Predicted Rating: 3/5\n",
      "Confidence: ['5.6%', '16.4%', '75.9%', '1.6%', '0.5%']\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "test_cards = [\"Abyssal Gorestalker\", \"Adventuring Gear\", \"Monstrous Rage\", \"Phyrexian Arena\", \"Abrade\"]\n",
    "\n",
    "for card in test_cards:\n",
    "    rating, confidence = predict_card_rating(card, model, tokenizer)\n",
    "    if rating is not None:\n",
    "        # Format confidence scores as percentages\n",
    "        conf_percentages = [f\"{conf*100:.1f}%\" for conf in confidence]   \n",
    "                 \n",
    "        print(f\"Card: {card}\")\n",
    "        print(f\"Predicted Rating: {rating}/5\")\n",
    "        print(f\"Confidence: {conf_percentages}\")\n",
    "        print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e7d0ed",
   "metadata": {},
   "source": [
    "### Overview:\n",
    "\n",
    "This method failed miserably, for a couple of reasons, both having to do with my data. As we can see, the model has defaulted to predicting 2 for almost every card, with a rare 3, and even though it is doing so, it is doing so without a lot of confidence in that answer. I believe that the reason for this is that the data is skewed towards the lower scores. This is the distribution of the data per score: 1: 17, 2: 36, 3: 34, 4: 11, 5: 2. We can see by looking at the above predictions that the probabilities given for each score \n",
    "\n",
    "Desipte many attempts at modifying hyperparameters such as epochs and learning rate to increase the amount of learning done, I could not bring the model to make different predictions for different cards. The best learning rate that I found was 2e-4. Any lower and the model simply stopped learning anything, and gave near 20 percent for each probability. The model seems to converge after 5 or so epochs each time, if the anti-learning that is being done in a lot of iterations can be called converging. Currently, I firmly believe that the miniscule dataset is preventing the model from learning trends that help it outside of the training data.\n",
    "\n",
    "As a consequence of the very tiny amount of data coupled with the skew, the model is learning to be less surprised that a results is a 2 or a 3 instead of actually learning something about classifying the cards. As a method of evaluating Magic cards, this fine-tuning attempt was a failure. However, as a learning experience about fine-tuning and a lesson in careful and extensive data collection, it was a success!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56db04",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In summary, I believe that while fine-tuning a model has potential for this problem, it is not within the timeframe that I have to collect enough data to make this a viable solution. With the small dataset that I was able to collect, and because of it's flaws, I was unable to effectively fine-tune the model. However, I learned a valuable lesson about both \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e78c21b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
