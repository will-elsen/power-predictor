{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e73935aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338bbfeb36554bcf80a18d879cade412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configuring model...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import predictor\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# login to Hugging Face Hub\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "# Configuration\n",
    "print(\"configuring model...\")\n",
    "MODEL_NAME = \"distilbert/distilbert-base-uncased\"\n",
    "OUTPUT_DIR = \"./mtg_rating_model\"\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "MAX_LENGTH = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a55e4cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample data structure (replace with your actual data)\n",
    "def load_sample_data():\n",
    "    \"\"\"Load sample data or replace with your actual data loading code\"\"\"\n",
    "    cards = [\n",
    "        \"Adventuring Gear\",\n",
    "        \"Basilisk Collar\",\n",
    "        \"Bloodthorn Flail\",\n",
    "        \"Carnelian Orb of Dragonkind\",\n",
    "        \"Carrot Cake\",\n",
    "        \"Cori-Steel Cutter\",\n",
    "        \"Gilded Lotus\",\n",
    "        \"Golden Argosy\",\n",
    "        \"Monument to Endurance\",\n",
    "        \"Perilous Snare\",\n",
    "        \"Racers' Scoreboard\",\n",
    "        \"Rope\",\n",
    "        \"Runaway Boulder\",\n",
    "        \"Abhorrent Oculus\",\n",
    "        \"Ajani's Pridemate\",\n",
    "        \"Ash, Party Crasher\",\n",
    "        \"Ball Lightning\",\n",
    "        \"Beza, the Bounding Spring\",\n",
    "        \"Bloodghast\",\n",
    "        \"Boulderborn Dragon\",\n",
    "        \"Brightblade Stoat\",\n",
    "        \"Defiler of Vigor\",\n",
    "        \"Diregraf Ghoul\",\n",
    "        \"Edgewall Pack\",\n",
    "        \"Elvish Archdruid\",\n",
    "        \"Essence Channeler\",\n",
    "        \"Evolved Sleeper\",\n",
    "        \"Fang Guardian\",\n",
    "        \"Fangkeeper's Familiar\",\n",
    "        \"Friendly Teddy\",\n",
    "        \"Fynn, the Fangbearer\",\n",
    "        \"Greedy Freebooter\",\n",
    "        \"Halo-Charged Skaab\",\n",
    "        \"Haughty Djinn\",\n",
    "        \"Heartfire Hero\",\n",
    "        \"Hinterland Sanctifier\",\n",
    "        \"Ingenious Leonin\",\n",
    "        \"Iridescent Vinelasher\",\n",
    "        \"Jolly Gerbils\",\n",
    "        \"Kiora, the Rising Tide\",\n",
    "        \"Knight-Errant of Eos\",\n",
    "        \"Kraul Whipcracker\",\n",
    "        \"Llanowar Elves\",\n",
    "        \"Manifold Mouse\",\n",
    "        \"Mintstrosity\",\n",
    "        \"Nurturing Pixie\",\n",
    "        \"Overlord of the Hauntwoods\",\n",
    "        \"Overlord of the Boilerbilges\",\n",
    "        \"Pride of the Road\",\n",
    "        \"Rankle and Torbran\",\n",
    "        \"Savage Ventmaw\",\n",
    "        \"Savannah Lions\",\n",
    "        \"Screaming Nemesis\",\n",
    "        \"Severance Priest\",\n",
    "        \"Sire of Seven Deaths\",\n",
    "        \"Skirmish Rhino\",\n",
    "        \"Spiteful Hexmage\",\n",
    "        \"Tangled Colony\",\n",
    "        \"Up the Beanstalk\",\n",
    "        \"Caretaker's Talent\",\n",
    "        \"Colossification\",\n",
    "        \"Disturbing Mirth\",\n",
    "        \"Leyline of Resonance\",\n",
    "        \"Lost in the Maze\",\n",
    "        \"Nahiri's Resolve\",\n",
    "        \"Nowhere to Run\",\n",
    "        \"Phyrexian Arena\",\n",
    "        \"Tribute to the World Tree\",\n",
    "        \"Monstrous Rage\",\n",
    "        \"Abrade\",\n",
    "        \"Aetherize\",\n",
    "        \"Bite Down\",\n",
    "        \"Flame Lash\",\n",
    "        \"Get Out\",\n",
    "        \"Get Lost\",\n",
    "        \"This Town Ain't Big Enough\",\n",
    "        \"Negate\",\n",
    "        \"On the Job\",\n",
    "        \"Opt\",\n",
    "        \"Rat Out\",\n",
    "        \"Refute\",\n",
    "        \"Ride's End\",\n",
    "        \"Slick Sequence\",\n",
    "        \"Steer Clear\",\n",
    "        \"Torch the Tower\",\n",
    "        \"Abuelo's Awakening\",\n",
    "        \"Boltwave\",\n",
    "        \"Captain's Call\",\n",
    "        \"Deathmark\",\n",
    "        \"Excavation Explosion\",\n",
    "        \"Exorcise\",\n",
    "        \"Feed the Swarm\",\n",
    "        \"Jailbreak Scheme\",\n",
    "        \"Lunar Insight\",\n",
    "        \"Maelstrom Pulse\",\n",
    "        \"Pyroclasm\",\n",
    "        \"Rankle's Prank\",\n",
    "        \"Zombify\",\n",
    "        \"Slime Against Humanity\",\n",
    "        \"Sunfall\"\n",
    "    ]\n",
    "    \n",
    "    # fetch data from scryfall and append to cards_with_data for use in training\n",
    "    cards_with_data = []\n",
    "    for card in cards:\n",
    "        card_info = predictor.card_utils.get_card_info(card)\n",
    "        with_data = f\"Name: {card_info['name']}\\nMana Cost: {card_info['mana_cost']}\\nTypes: {card_info['types']}\\nOracle Text: {card_info['oracle_text']}\\nPower/Toughness: {card_info['power']}/{card_info['toughness']}\\nLoyalty: {card_info['loyalty']}\\n\\n.\"\n",
    "        cards_with_data.append(with_data)\n",
    "        \n",
    "    # Ratings for example (1-5 scale)\n",
    "    ratings = [1, 2, 1, 2, 3, 4, 2, 2, 4, 3, 1, 2, 1, 4, 2, 2, 2, 4, 3, 1, 2, 3, 2, 2, 3, 3, 2, 1, 3, 1, 3, 3, 2, 3, 4, 3, 1, 3, 1, 3, 4, 2, 4, 3, 2, 3, 3, 4, 1, 2, 2, 2, 3, 2, 2, 3, 2, 2, 4, 1, 3, 4, 1, 1, 3, 3, 3, 5, 2, 3, 2, 1, 3, 3, 5, 3, 1, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 1, 2, 2, 1, 2, 3, 3, 2, 2, 4, 3]\n",
    "    \n",
    "    return pd.DataFrame({\"card_text\": cards_with_data, \"rating\": ratings})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00abc580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 17, 2: 36, 3: 34, 4: 11, 5: 2\n"
     ]
    }
   ],
   "source": [
    "ratings = [1, 2, 1, 2, 3, 4, 2, 2, 4, 3, 1, 2, 1, 4, 2, 2, 2, 4, 3, 1, 2, 3, 2, 2, 3, 3, 2, 1, 3, 1, 3, 3, 2, 3, 4, 3, 1, 3, 1, 3, 4, 2, 4, 3, 2, 3, 3, 4, 1, 2, 2, 2, 3, 2, 2, 3, 2, 2, 4, 1, 3, 4, 1, 1, 3, 3, 3, 5, 2, 3, 2, 1, 3, 3, 5, 3, 1, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 1, 2, 2, 1, 2, 3, 3, 2, 2, 4, 3]\n",
    "one = 0\n",
    "two = 0\n",
    "three = 0\n",
    "four = 0\n",
    "five = 0\n",
    "for rating in ratings:\n",
    "    if rating == 1:\n",
    "        one += 1\n",
    "    elif rating == 2:\n",
    "        two += 1\n",
    "    elif rating == 3:\n",
    "        three += 1\n",
    "    elif rating == 4:\n",
    "        four += 1\n",
    "    elif rating == 5:\n",
    "        five += 1\n",
    "print(f\"1: {one}, 2: {two}, 3: {three}, 4: {four}, 5: {five}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eda85a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize the data for sequence classification\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    \"\"\"Tokenize examples for sequence classification\"\"\"\n",
    "    # Format the examples as input text\n",
    "    texts = [\n",
    "        f\"Rate the Magic: The Gathering card '{card_text}' on a scale from 1 to 5 where 1 is irrelevant to the current standard format and 5 is format-warping.\"\n",
    "        for card_text in examples[\"card_text\"]\n",
    "    ]\n",
    "    \n",
    "    # Tokenize with padding and truncation\n",
    "    tokenized = tokenizer(\n",
    "        texts, \n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Convert ratings to labels (subtract 1 to make labels 0-4 instead of 1-5)\n",
    "    tokenized[\"labels\"] = [label - 1 for label in examples[\"rating\"]]\n",
    "    \n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a68bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    print(\"Loading sample data...\")\n",
    "    df = load_sample_data()\n",
    "    \n",
    "    print(f\"Loaded {len(df)} card ratings.\")\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create HF datasets\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    print(f\"Loading {MODEL_NAME}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Make sure the tokenizer has a pad token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load the model with num_labels=5 for the 1-5 rating scale\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=5,  # 5 classes (ratings 1-5)\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Configure LoRA\n",
    "    peft_config = LoraConfig(\n",
    "        inference_mode=False,\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS,  # Sequence classification task\n",
    "        target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"o_lin\"]  # Typical attention modules\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to the model\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    print(\"Preparing datasets...\")\n",
    "    \n",
    "    # Apply tokenization function to datasets\n",
    "    def tokenize_dataset(examples):\n",
    "        return tokenize_function(examples, tokenizer)\n",
    "    \n",
    "    # Process datasets with batched=True for efficiency\n",
    "    tokenized_train = train_dataset.map(\n",
    "        tokenize_dataset,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    tokenized_val = val_dataset.map(\n",
    "        tokenize_dataset,\n",
    "        batched=True,\n",
    "        remove_columns=val_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.03,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        push_to_hub=False,\n",
    "        fp16=False,\n",
    "    )\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        label_names=[\"rating\"],\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the model\n",
    "    print(f\"Saving model to {OUTPUT_DIR}...\")\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    \n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    # Example using the trainer.state.log_history\n",
    "    train_loss = []\n",
    "    eval_loss = []\n",
    "    for step_data in trainer.state.log_history:\n",
    "        if 'loss' in step_data:\n",
    "            train_loss.append(step_data['loss'])\n",
    "        if 'eval_loss' in step_data:\n",
    "            eval_loss.append(step_data['eval_loss'])\n",
    "            \n",
    "    steps = range(len(train_loss))\n",
    "    plt.plot(steps, train_loss, label='Train Loss')\n",
    "    plt.plot(steps, eval_loss, label='Eval Loss')\n",
    "    \n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Evaluation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f73a500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sample data...\n",
      "Loaded 100 card ratings.\n",
      "Loading distilbert/distilbert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,036,805 || all params: 67,994,122 || trainable%: 1.5248\n",
      "Preparing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2092827cc2084c5a8ba41cdcc4417934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531ee5df585845f09d58375c728a9c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Trainer.__init__() got an unexpected keyword argument 'label_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 88\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     68\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mOUTPUT_DIR,\n\u001b[1;32m     69\u001b[0m     overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Initialize Trainer\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting fine-tuning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Trainer.__init__() got an unexpected keyword argument 'label_names'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
