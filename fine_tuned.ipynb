{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e73935aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d094c7fbcb40029bb5ddcc701b13d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configuring model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import predictor\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "# login to Hugging Face Hub\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "# Configuration\n",
    "print(\"configuring model...\")\n",
    "MODEL_NAME = \"distilbert/distilbert-base-uncased\"\n",
    "OUTPUT_DIR = \"./mtg_rating_model\"\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "MAX_LENGTH = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a55e4cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample data structure (replace with your actual data)\n",
    "def load_sample_data():\n",
    "    \"\"\"Load sample data or replace with your actual data loading code\"\"\"\n",
    "    cards = [\n",
    "        \"Adventuring Gear\",\n",
    "        \"Basilisk Collar\",\n",
    "        \"Bloodthorn Flail\",\n",
    "        \"Carnelian Orb of Dragonkind\",\n",
    "        \"Carrot Cake\",\n",
    "        \"Cori-Steel Cutter\",\n",
    "        \"Gilded Lotus\",\n",
    "        \"Golden Argosy\",\n",
    "        \"Monument to Endurance\",\n",
    "        \"Perilous Snare\",\n",
    "        \"Racers' Scoreboard\",\n",
    "        \"Rope\",\n",
    "        \"Runaway Boulder\",\n",
    "        \"Abhorrent Oculus\",\n",
    "        \"Ajani's Pridemate\",\n",
    "        \"Ash, Party Crasher\",\n",
    "        \"Ball Lightning\",\n",
    "        \"Beza, the Bounding Spring\",\n",
    "        \"Bloodghast\",\n",
    "        \"Boulderborn Dragon\",\n",
    "        \"Brightblade Stoat\",\n",
    "        \"Defiler of Vigor\",\n",
    "        \"Diregraf Ghoul\",\n",
    "        \"Edgewall Pack\",\n",
    "        \"Elvish Archdruid\",\n",
    "        \"Essence Channeler\",\n",
    "        \"Evolved Sleeper\",\n",
    "        \"Fang Guardian\",\n",
    "        \"Fangkeeper's Familiar\",\n",
    "        \"Friendly Teddy\",\n",
    "        \"Fynn, the Fangbearer\",\n",
    "        \"Greedy Freebooter\",\n",
    "        \"Halo-Charged Skaab\",\n",
    "        \"Haughty Djinn\",\n",
    "        \"Heartfire Hero\",\n",
    "        \"Hinterland Sanctifier\",\n",
    "        \"Ingenious Leonin\",\n",
    "        \"Iridescent Vinelasher\",\n",
    "        \"Jolly Gerbils\",\n",
    "        \"Kiora, the Rising Tide\",\n",
    "        \"Knight-Errant of Eos\",\n",
    "        \"Kraul Whipcracker\",\n",
    "        \"Llanowar Elves\",\n",
    "        \"Manifold Mouse\",\n",
    "        \"Mintstrosity\",\n",
    "        \"Nurturing Pixie\",\n",
    "        \"Overlord of the Hauntwoods\",\n",
    "        \"Overlord of the Boilerbilges\",\n",
    "        \"Pride of the Road\",\n",
    "        \"Rankle and Torbran\",\n",
    "        \"Savage Ventmaw\",\n",
    "        \"Savannah Lions\",\n",
    "        \"Screaming Nemesis\",\n",
    "        \"Severance Priest\",\n",
    "        \"Sire of Seven Deaths\",\n",
    "        \"Skirmish Rhino\",\n",
    "        \"Spiteful Hexmage\",\n",
    "        \"Tangled Colony\",\n",
    "        \"Up the Beanstalk\",\n",
    "        \"Caretaker's Talent\",\n",
    "        \"Colossification\",\n",
    "        \"Disturbing Mirth\",\n",
    "        \"Leyline of Resonance\",\n",
    "        \"Lost in the Maze\",\n",
    "        \"Nahiri's Resolve\",\n",
    "        \"Nowhere to Run\",\n",
    "        \"Phyrexian Arena\",\n",
    "        \"Tribute to the World Tree\",\n",
    "        \"Monstrous Rage\",\n",
    "        \"Abrade\",\n",
    "        \"Aetherize\",\n",
    "        \"Bite Down\",\n",
    "        \"Flame Lash\",\n",
    "        \"Get Out\",\n",
    "        \"Get Lost\",\n",
    "        \"This Town Ain't Big Enough\",\n",
    "        \"Negate\",\n",
    "        \"On the Job\",\n",
    "        \"Opt\",\n",
    "        \"Rat Out\",\n",
    "        \"Refute\",\n",
    "        \"Ride's End\",\n",
    "        \"Slick Sequence\",\n",
    "        \"Steer Clear\",\n",
    "        \"Torch the Tower\",\n",
    "        \"Abuelo's Awakening\",\n",
    "        \"Boltwave\",\n",
    "        \"Captain's Call\",\n",
    "        \"Deathmark\",\n",
    "        \"Excavation Explosion\",\n",
    "        \"Exorcise\",\n",
    "        \"Feed the Swarm\",\n",
    "        \"Jailbreak Scheme\",\n",
    "        \"Lunar Insight\",\n",
    "        \"Maelstrom Pulse\",\n",
    "        \"Pyroclasm\",\n",
    "        \"Rankle's Prank\",\n",
    "        \"Zombify\",\n",
    "        \"Slime Against Humanity\",\n",
    "        \"Sunfall\"\n",
    "    ]\n",
    "    \n",
    "    # fetch data from scryfall and append to cards_with_data for use in training\n",
    "    cards_with_data = []\n",
    "    for card in cards:\n",
    "        card_info = predictor.card_utils.get_card_info(card)\n",
    "        with_data = f\"Name: {card_info['name']}\\nMana Cost: {card_info['mana_cost']}\\nTypes: {card_info['types']}\\nOracle Text: {card_info['oracle_text']}\\nPower/Toughness: {card_info['power']}/{card_info['toughness']}\\nLoyalty: {card_info['loyalty']}\\n\\n.\"\n",
    "        cards_with_data.append(with_data)\n",
    "        \n",
    "    # Ratings for example (1-5 scale)\n",
    "    ratings = [1, 2, 1, 2, 3, 4, 2, 2, 4, 3, 1, 2, 1, 4, 2, 2, 2, 4, 3, 1, 2, 3, 2, 2, 3, 3, 2, 1, 3, 1, 3, 3, 2, 3, 4, 3, 1, 3, 1, 3, 4, 2, 4, 3, 2, 3, 3, 4, 1, 2, 2, 2, 3, 2, 2, 3, 2, 2, 4, 1, 3, 4, 1, 1, 3, 3, 3, 5, 2, 3, 2, 1, 3, 3, 5, 3, 1, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 1, 2, 2, 1, 2, 3, 3, 2, 2, 4, 3]\n",
    "    \n",
    "    return pd.DataFrame({\"card_text\": cards_with_data, \"rating\": ratings})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00abc580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 17, 2: 36, 3: 34, 4: 11, 5: 2\n"
     ]
    }
   ],
   "source": [
    "ratings = [1, 2, 1, 2, 3, 4, 2, 2, 4, 3, 1, 2, 1, 4, 2, 2, 2, 4, 3, 1, 2, 3, 2, 2, 3, 3, 2, 1, 3, 1, 3, 3, 2, 3, 4, 3, 1, 3, 1, 3, 4, 2, 4, 3, 2, 3, 3, 4, 1, 2, 2, 2, 3, 2, 2, 3, 2, 2, 4, 1, 3, 4, 1, 1, 3, 3, 3, 5, 2, 3, 2, 1, 3, 3, 5, 3, 1, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 1, 2, 2, 1, 2, 3, 3, 2, 2, 4, 3]\n",
    "one = 0\n",
    "two = 0\n",
    "three = 0\n",
    "four = 0\n",
    "five = 0\n",
    "for rating in ratings:\n",
    "    if rating == 1:\n",
    "        one += 1\n",
    "    elif rating == 2:\n",
    "        two += 1\n",
    "    elif rating == 3:\n",
    "        three += 1\n",
    "    elif rating == 4:\n",
    "        four += 1\n",
    "    elif rating == 5:\n",
    "        five += 1\n",
    "print(f\"1: {one}, 2: {two}, 3: {three}, 4: {four}, 5: {five}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eda85a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize the data for sequence classification\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    \"\"\"Tokenize examples for sequence classification\"\"\"\n",
    "    # Format the examples as input text\n",
    "    texts = [\n",
    "        f\"Rate the Magic: The Gathering card '{card_text}' on a scale from 1 to 5 where 1 is irrelevant to the current standard format and 5 is format-warping.\"\n",
    "        for card_text in examples[\"card_text\"]\n",
    "    ]\n",
    "    \n",
    "    # Tokenize with padding and truncation\n",
    "    tokenized = tokenizer(\n",
    "        texts, \n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Convert ratings to labels (subtract 1 to make labels 0-4 instead of 1-5)\n",
    "    tokenized[\"labels\"] = [label - 1 for label in examples[\"rating\"]]\n",
    "    \n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a68bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    print(\"Loading sample data...\")\n",
    "    df = load_sample_data()\n",
    "    \n",
    "    print(f\"Loaded {len(df)} card ratings.\")\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create HF datasets\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    print(f\"Loading {MODEL_NAME}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Make sure the tokenizer has a pad token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load the model with num_labels=5 for the 1-5 rating scale\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=5,  # 5 classes (ratings 1-5)\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Configure LoRA\n",
    "    peft_config = LoraConfig(\n",
    "        inference_mode=False,\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS,  # Sequence classification task\n",
    "        target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"o_lin\"]  # Typical attention modules\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to the model\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    print(\"Preparing datasets...\")\n",
    "    \n",
    "    # Apply tokenization function to datasets\n",
    "    def tokenize_dataset(examples):\n",
    "        return tokenize_function(examples, tokenizer)\n",
    "    \n",
    "    # Process datasets with batched=True for efficiency\n",
    "    tokenized_train = train_dataset.map(\n",
    "        tokenize_dataset,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    tokenized_val = val_dataset.map(\n",
    "        tokenize_dataset,\n",
    "        batched=True,\n",
    "        remove_columns=val_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.03,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        push_to_hub=False,\n",
    "        fp16=False,\n",
    "    )\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the model\n",
    "    print(f\"Saving model to {OUTPUT_DIR}...\")\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    \n",
    "    \n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f73a500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sample data...\n",
      "Loaded 100 card ratings.\n",
      "Loading distilbert/distilbert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,036,805 || all params: 67,994,122 || trainable%: 1.5248\n",
      "Preparing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3639642988a34a9a8547096f1ae497fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01231abb19b4446a8ae960fbb5d9618a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.651600</td>\n",
       "      <td>1.623438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./mtg_rating_model...\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
